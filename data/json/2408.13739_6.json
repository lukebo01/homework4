{
    "table": [
        [
            "nan",
            "LT as LT",
            "LT as CT",
            "CT as LT",
            "CT as CT"
        ],
        [
            "GMM",
            "0.88",
            "0.12",
            "0.13",
            "0.87"
        ],
        [
            "CNN",
            "0.90",
            "0.10",
            "0.02",
            "0.98"
        ],
        [
            "PPR",
            "0.89",
            "0.11",
            "0.11",
            "0.89"
        ],
        [
            "P-LVCSR",
            "0.95",
            "0.05",
            "0.06",
            "0.94"
        ],
        [
            "UPR-1",
            "0.96",
            "0.04",
            "0.09",
            "0.91"
        ],
        [
            "UPR-2",
            "0.97",
            "0.03",
            "0.06",
            "0.94"
        ]
    ],
    "caption": "Table 6:  Confusions in each method",
    "references": [
        "Table 6 shows the confusions in all six techniques.\nFrom the table we can infer that,\na lot of confusions happen for colloquial Tamil. The reason\nis that, the average duration of a CT utterance is shorter in comparison\n[26]. If utterances with similar lengths, or a\nparallel corpus is available, this factor could be overlooked and a\nbetter analysis could be provided. An unique result can be observed for\nthe case of CNN. Here we see that CNN handles CT better than all other\nmethods. This could be because of the fact that CNNs learn the full\nutterance as a whole, rather than learning it frame by frame or, one\nphone label after the other as in the case of GMM and the other methods\nrespectively. This process could have captured the wholesome acoustic\ncharacteristics of the dialect. Another probable reason is that while\npre-processing the utterances for CNN, the size of the data is kept\nconstant by zero padding or truncating it. Hence removing the bias that\nthe length of the sentence offers, as mentioned earlier in the case of\nother methods."
    ]
}