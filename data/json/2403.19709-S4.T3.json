{
    "table": [
        [
            "Model",
            "# Params.",
            "Mean",
            "Paired T-Test"
        ],
        [
            "Linear Head HRA",
            "51M",
            "10.6",
            "-"
        ],
        [
            "Linear Head HRA",
            "102M",
            "10.9",
            "-"
        ],
        [
            "Linear Head HRA",
            "203M",
            "11.0",
            "-"
        ],
        [
            "FFN Head HRA",
            "201M",
            "9.9",
            "-"
        ],
        [
            "FFN Head HRA",
            "403M",
            "10.2",
            "-"
        ],
        [
            "FFN Head HRA",
            "806M",
            "10.4",
            "-"
        ],
        [
            "Linear Head HRA (w/ pre-trained controller)",
            "51M",
            "10.7",
            "0.59"
        ],
        [
            "Linear Head HRA (w/ pre-trained controller)",
            "101M",
            "11.0",
            "0.25"
        ],
        [
            "Linear Head HRA (w/ pre-trained controller)",
            "202M",
            "11.3",
            "0.03"
        ],
        [
            "FFN Head HRA (w/ pre-trained controller)",
            "118M",
            "10.1",
            "0.17"
        ],
        [
            "FFN Head HRA (w/ pre-trained controller)",
            "269M",
            "10.3",
            "0.14"
        ],
        [
            "FFN Head HRA (w/ pre-trained controller)",
            "672M",
            "10.5",
            "0.22"
        ]
    ],
    "caption": "Table 3 :  Online adaptation WER results on\nEuphonia\ndata sets. Our FFN Head HRA (S) with pre-trained controller achieves comparable results against the regular setup (only 0.2% WER loss). Paired T-Test shows no statistically significant difference between with and without pre-trained controller.",
    "references": [
        "Table 3 reports the WER results from our multi task adaptation experiments with and without pre-trained controller. We hand picked an extra 128\nEuphonia\nspeaker data as out-of-domain data with respect to the in-domain 128\nEuphonia\nspeaker data mentioned above. We divide the training into two steps. First step, we pre-train the recurrent controller network with out-of-domain data. Second step, we freeze the recurrent controller network, use in-domain data to train the adapter head with random initialization. So the number of actual training parameter is reduced in this setup as we only train the adapter head. Furthermore, this approach provides a solution for sensitive data sets that cannot be trained within one model. If we pre-train the recurrent controller network only on non-Personal Identifiable Information (PII) data, and parameterize the adapter head by speaker, then no speaker needs to share tuning parameters with others."
    ]
}