{
    "table": [
        [
            "LLMs",
            "Publish time",
            "Parameter quantity",
            "Pre-training data size",
            "Training paradigm",
            "Feature"
        ],
        [
            "GPT",
            "2018.7",
            "120 million",
            "5G",
            "Pre-training + fine-tuning",
            "Reflection of the advantages of self-attention structure"
        ],
        [
            "GPT-2",
            "2019.2",
            "1.5 billion",
            "40G",
            "Prompt paradigm based on Tunning-free: Zero Shot Prompt",
            "Open the exploration of the Prompt paradigm"
        ],
        [
            "GPT-3",
            "2020.6",
            "175 billion",
            "45TB",
            "Prompt paradigm based on Tunning-free: In-Context Learning",
            "Deepen the exploration of the Prompt paradigm"
        ],
        [
            "InstructGPT",
            "2022.3",
            "175 billion",
            "45TB",
            "Prompt paradigm of Instruction Tuning",
            "Start paying attention to human preferences"
        ],
        [
            "ChatGPT",
            "2022.11",
            "175 billion",
            "45TB",
            "Reinforcement learning from human feedback",
            "Aligned with human preferences"
        ],
        [
            "GPT-4",
            "2023.3",
            "Nearly 2 trillion",
            "-",
            "Reinforcement learning from human feedback",
            "Multimodal processing and getting closer to the bionic human brain"
        ],
        [
            "LaMDA",
            "2021",
            "137 billion",
            "150TB",
            "Pre-training + fine-tuning",
            "Introduce external information retrieval system"
        ],
        [
            "BARD",
            "2023.2",
            "137 billion",
            "-",
            "Join ChromeOS as a search engine",
            "Using LaMDA as a base"
        ],
        [
            "PaLM",
            "2022.4",
            "540 billion",
            "-",
            "PathWay distributed training framework",
            "Large scale, multi-lingual"
        ],
        [
            "Claude",
            "2023.3",
            "52 billion",
            "-",
            "Join the RLAIF training paradigm",
            "Longer and more natural text editing than ChatGPT"
        ],
        [
            "BlenderBot3",
            "2022.8",
            "175 billion",
            "-",
            "Instruction fine-tuning",
            "Text generation, question answering"
        ]
    ],
    "caption": "",
    "references": []
}