{
    "table": [
        [
            "Dataset",
            "Data type",
            "Label type",
            "Hours"
        ],
        [
            "ICMC",
            "Far+Near+speed3",
            "supervised",
            "386"
        ],
        [
            "ICMC+addNoise",
            "Far+speed3",
            "supervised",
            "1544"
        ],
        [
            "3D-SPEAKER",
            "Near",
            "unsupervised",
            "1124"
        ],
        [
            "AliMeeting",
            "Far+Near",
            "unsupervised",
            "236"
        ],
        [
            "AISHELL-4",
            "Far+Near",
            "unsupervised",
            "240"
        ],
        [
            "Aidatatang",
            "Near",
            "unsupervised",
            "200"
        ],
        [
            "MagicData",
            "Near",
            "(text is not",
            "180"
        ],
        [
            "KeSpeech",
            "Near",
            "allowed to use)",
            "1542"
        ],
        [
            "WenetSpeech",
            "Near",
            "nan",
            "5500"
        ],
        [
            "WenetSpeech",
            "(drama, talk,",
            "nan",
            "5500"
        ],
        [
            "WenetSpeech",
            "interview)",
            "nan",
            "5500"
        ]
    ],
    "caption": "Table 1 :  Training Data Description.",
    "references": [
        "Data resource:\nAll data resources used for the proposed system are summarized in Table 1, which include two categories. One originates from the official ICMC-ASR labelled data [2], and the other from the external openSLR unlabelled data following the official rules [3]. For the far-field data, the single-channel audio is extracted by oracle diarization and GSS. For unsupervised data, we propose an iterative pseudo-label generation (PLG) method based on the fusion ASR model. To enhance the generalization of the fusion model, we consider different input features and encoder structures: 1) conformer encoder-decoder (ED) model based on self-supervised learning representation (SSLR), and 2) ebranchformer ED model using Fbank. The former is employed to adapt long-duration audio, while the latter is used for short-duration counterparts. The SSLR is extracted by the adaptive wavLM model [1], which is trained using the WenetSpeech full 2.5wh dataset. For pseudo-label generation, a small amount of supervised data B0subscript\ud835\udc350B_{0} is initially used to train and fuse into P\u200bL\u200bG0\ud835\udc43\ud835\udc3fsubscript\ud835\udc3a0PLG_{0}. The unsupervised data is then split into different batches, say B1,\u2026,BNsubscript\ud835\udc351\u2026subscript\ud835\udc35\ud835\udc41B_{1},...,B_{N}. Subsequently, the pseudo-labels for the unsupervised data in BNsubscript\ud835\udc35\ud835\udc41B_{N} are generated by P\u200bL\u200bGN\u22121\ud835\udc43\ud835\udc3fsubscript\ud835\udc3a\ud835\udc411PLG_{N-1} in a cyclic iterative fashion, and P\u200bL\u200bGN\ud835\udc43\ud835\udc3fsubscript\ud835\udc3a\ud835\udc41PLG_{N} is trained and fused by BNsubscript\ud835\udc35\ud835\udc41B_{N}. As such, we can obtain 10,952 hours of data for ASR training in total.",
        "Speaker Diarization:\nThe speaker diarization system trained on 1544 hours data in Table 1 consists of three components: multi-channel voice activity detection (VAD), clustering-based speaker diarization (CSD) and multi-channel target-speaker VAD (MC-TS-VAD) [1]. Due to the additional non-target speaker (recorder) in the official data, traditional i-vector can not effectively distinguish between target and non-target speakers. To improve the generalization of speaker embeddings, we consider the fusion of the SSLR-based x-vector for the i-vector in the MC-TS-VAD module. This embedding fusion enables the diarization system to fully exploit speaker information and is thus helpful for speaker separation."
    ]
}