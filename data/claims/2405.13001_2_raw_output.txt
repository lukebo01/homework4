{
  "0": {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "Publish time"},
      "2": {"name": "header", "value": "Parameter quantity"},
      "3": {"name": "header", "value": "Pre-training data size"},
      "4": {"name": "header", "value": "Training paradigm"},
      "5": {"name": "header", "value": "Feature"}
    },
    "Measure": "Publish time",
    "Outcome": "2018.7"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2018.7"},
      "2": {"name": "header", "value": "120 million"},
      "3": {"name": "header", "value": "5G"},
      "4": {"name": "header", "value": "Pre-training + fine-tuning"},
      "5": {"name": "header", "value": "Reflection of the advantages of self-attention structure"}
    },
    "Measure": "Parameter quantity",
    "Outcome": "120 million"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2019.2"},
      "2": {"name": "header", "value": "1.5 billion"},
      "3": {"name": "header", "value": "40G"},
      "4": {"name": "header", "value": "Prompt paradigm based on Tunning-free: Zero Shot Prompt"},
      "5": {"name": "header", "value": "Open the exploration of the Prompt paradigm"}
    },
    "Measure": "Pre-training data size",
    "Outcome": "1.5 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2020.6"},
      "2": {"name": "header", "value": "175 billion"},
      "3": {"name": "header", "value": "45TB"},
      "4": {"name": "header", "value": "Prompt paradigm based on Tunning-free: In-Context Learning"},
      "5": {"name": "header", "value": "Deepen the exploration of the Prompt paradigm"}
    },
    "Measure": "Training paradigm",
    "Outcome": "175 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2022.3"},
      "2": {"name": "header", "value": "175 billion"},
      "3": {"name": "header", "value": "45TB"},
      "4": {"name": "header", "value": "Prompt paradigm of Instruction Tuning"},
      "5": {"name": "header", "value": "Start paying attention to human preferences"}
    },
    "Measure": "Pre-training data size",
    "Outcome": "175 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2022.11"},
      "2": {"name": "header", "value": "175 billion"},
      "3": {"name": "header", "value": "45TB"},
      "4": {"name": "header", "value": "Reinforcement learning from human feedback"},
      "5": {"name": "header", "value": "Aligned with human preferences"}
    },
    "Measure": "Training paradigm",
    "Outcome": "175 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2023.3"},
      "2": {"name": "header", "value": "Nearly 2 trillion"},
      "3": {"name": "header", "value": "-"},
      "4": {"name": "header", "value": "Reinforcement learning from human feedback"},
      "5": {"name": "header", "value": "Aligned with human preferences"}
    },
    "Measure": "Pre-training data size",
    "Outcome": "Nearly 2 trillion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2019.2"},
      "2": {"name": "header", "value": "120 million"},
      "3": {"name": "header", "value": "5G"},
      "4": {"name": "header", "value": "Pre-training + fine-tuning"},
      "5": {"name": "header", "value": "Reflection of the advantages of self-attention structure"}
    },
    "Measure": "Parameter quantity",
    "Outcome": "120 million"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2022.3"},
      "2": {"name": "header", "value": "175 billion"},
      "3": {"name": "header", "value": "45TB"},
      "4": {"name": "header", "value": "Prompt paradigm of Instruction Tuning"},
      "5": {"name": "header", "value": "Start paying attention to human preferences"}
    },
    "Measure": "Pre-training data size",
    "Outcome": "175 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2022.11"},
      "2": {"name": "header", "value": "175 billion"},
      "3": {"name": "header", "value": "45TB"},
      "4": {"name": "header", "value": "Reinforcement learning from human feedback"},
      "5": {"name": "header", "value": "Aligned with human preferences"}
    },
    "Measure": "Training paradigm",
    "Outcome": "175 billion"
  },
  {
    "specifications": {
      "0": {"name": "header", "value": "LLMs"},
      "1": {"name": "header", "value": "2023.3"},
      "2": {"name": "header", "value": "Nearly 2 trillion"},
      "3": {"name": "header", "value": "-"},
      "4": {"name": "header", "value": "Reinforcement learning from human feedback"},
      "5": {"name": "header", "value": "Aligned with human preferences"}
    },
    "Measure": "Training paradigm",
    "Outcome": "Nearly 2 trillion"
  }
}